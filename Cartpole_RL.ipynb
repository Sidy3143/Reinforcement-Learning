{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGxrpHSmZqPGvi4uNYnkoO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sidy3143/Reinforcement-Learning/blob/main/Cartpole_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpp1C_RgPwL1"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Define the policy network\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.softmax(self.fc3(x), dim=-1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose action based on the policy\n",
        "def select_action(policy_net, state):\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "    probs = policy_net(state)\n",
        "    action = np.random.choice(np.arange(probs.shape[1]), p=probs.detach().numpy().ravel())\n",
        "    return action"
      ],
      "metadata": {
        "id": "SBsRJjkDouGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute discounted rewards\n",
        "def compute_returns(rewards, gamma=0.99):\n",
        "    R = 0\n",
        "    returns = []\n",
        "    for r in reversed(rewards):\n",
        "        R = r + gamma * R\n",
        "        returns.insert(0, R)\n",
        "    returns = torch.tensor(returns)\n",
        "    returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
        "    return returns"
      ],
      "metadata": {
        "id": "KOkfVEqSoyoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "hidden_dim = 64\n",
        "learning_rate = 1e-2\n",
        "gamma = 0.99\n",
        "batch_size = 20\n",
        "max_time_steps = int(1e6)\n",
        "save_interval = 400\n",
        "eval_steps = 1000"
      ],
      "metadata": {
        "id": "O2VClE2Mo0ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "policy_net = PolicyNetwork(env.observation_space.shape[0], hidden_dim, env.action_space.n)\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "TcpCx_-7o3Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "total_time_steps = 0\n",
        "episode_rewards = []\n",
        "\n",
        "while total_time_steps < max_time_steps:\n",
        "    state = env.reset()\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "\n",
        "    for t in range(100):  # Training loop for 100 time steps\n",
        "        action = select_action(policy_net, state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        log_prob = torch.log(policy_net(torch.from_numpy(state).float().unsqueeze(0))[0, action])\n",
        "        log_probs.append(log_prob)\n",
        "        rewards.append(reward)\n",
        "\n",
        "        state = next_state\n",
        "        total_time_steps += 1\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        # Every 20 time steps, perform a policy update\n",
        "        if total_time_steps % batch_size == 0:\n",
        "            returns = compute_returns(rewards)\n",
        "            loss = []\n",
        "            for log_prob, R in zip(log_probs, returns):\n",
        "                loss.append(-log_prob * R)\n",
        "            loss = torch.cat(loss).sum()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    episode_rewards.append(sum(rewards))\n",
        "\n",
        "    # Save and evaluate the model every 400 time steps\n",
        "    if total_time_steps % save_interval == 0:\n",
        "        torch.save(policy_net.state_dict(), f'policy_net_{total_time_steps}.pth')\n",
        "        print(f\"Model saved at time step {total_time_steps}\")\n",
        "\n",
        "        # Evaluation without training\n",
        "        eval_rewards = []\n",
        "        for _ in range(10):  # Run multiple test episodes\n",
        "            state = env.reset()\n",
        "            test_rewards = 0\n",
        "            for _ in range(eval_steps):  # Play for 1000 steps\n",
        "                action = select_action(policy_net, state)\n",
        "                state, reward, done, _ = env.step(action)\n",
        "                test_rewards += reward\n",
        "                if done:\n",
        "                    break\n",
        "            eval_rewards.append(test_rewards)\n",
        "        print(f\"Evaluation after {total_time_steps} time steps: Average reward = {np.mean(eval_rewards)}\")\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "id": "A0tKvwXmoqP1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}