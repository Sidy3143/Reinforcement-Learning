{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPc0uocPV5uDQ7A27yz6Z3o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sidy3143/Reinforcement-Learning/blob/main/actor_critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "import gym\n",
        "from gym import wrappers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "4-bDXyi0Zr1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# network\n",
        "class ActorCriticNetwork(keras.Model):\n",
        "    def __init__(self, n_actions, fc1_dims=1024, fc2_dims=512,\n",
        "            name='actor_critic', chkpt_dir='tmp/actor_critic'):\n",
        "        super(ActorCriticNetwork, self).__init__()\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.n_actions = n_actions\n",
        "        self.model_name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name + '_ac.weights.h5')\n",
        "\n",
        "        self.fc1 = Dense(self.fc1_dims, activation='relu')\n",
        "        self.fc2 = Dense(self.fc2_dims, activation='relu')\n",
        "        self.v = Dense(1, activation=None)\n",
        "        self.pi = Dense(n_actions, activation='softmax')\n",
        "\n",
        "    def call(self, state):\n",
        "        value = self.fc1(state)\n",
        "        value = self.fc2(value)\n",
        "\n",
        "        v = self.v(value)\n",
        "        pi = self.pi(value)\n",
        "\n",
        "        return v, pi"
      ],
      "metadata": {
        "id": "_lyMjbX43kWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#actor_critic Agent\n",
        "class Agent:\n",
        "    def __init__(self, alpha=0.0003, gamma=0.99, n_actions=2):\n",
        "        self.gamma = gamma\n",
        "        self.n_actions = n_actions\n",
        "        self.action = None\n",
        "        self.action_space = [i for i in range(self.n_actions)]\n",
        "\n",
        "        self.actor_critic = ActorCriticNetwork(n_actions=n_actions)\n",
        "        self.actor_critic.compile(optimizer=Adam(learning_rate=alpha))\n",
        "\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        state = tf.convert_to_tensor([observation])\n",
        "        _, probs = self.actor_critic(state)\n",
        "\n",
        "        action_probabilities = tfp.distributions.Categorical(probs=probs)\n",
        "        action = action_probabilities.sample()\n",
        "        log_prob = action_probabilities.log_prob(action)\n",
        "        self.action = action\n",
        "\n",
        "        return action.numpy()[0]\n",
        "\n",
        "    def save_models(self):\n",
        "        print('... saving models ...')\n",
        "        self.actor_critic.save_weights(self.actor_critic.checkpoint_file)\n",
        "\n",
        "    def load_models(self):\n",
        "        print('... loading models ...')\n",
        "        self.actor_critic.load_weights(self.actor_critic.checkpoint_file)\n",
        "\n",
        "    def learn(self, state, reward, state_, done):\n",
        "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "        state_ = tf.convert_to_tensor([state_], dtype=tf.float32)\n",
        "        reward = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            state_value, probs = self.actor_critic(state)\n",
        "            state_value_, _ = self.actor_critic(state_)\n",
        "            state_value = tf.squeeze(state_value)\n",
        "            state_value_ = tf.squeeze(state_value_)\n",
        "\n",
        "            action_probs = tfp.distributions.Categorical(probs=probs)\n",
        "            log_prob = action_probs.log_prob(self.action)\n",
        "\n",
        "            delta = reward + self.gamma*state_value_*(1-int(done)) - state_value   # TD Error\n",
        "            actor_loss = -log_prob*delta\n",
        "            critic_loss = delta**2\n",
        "            total_loss = actor_loss + critic_loss\n",
        "        gradient = tape.gradient(total_loss, self.actor_critic.trainable_variables)\n",
        "        self.actor_critic.optimizer.apply_gradients(zip(gradient, self.actor_critic.trainable_variables))"
      ],
      "metadata": {
        "id": "3AnZRnrA1haD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curve(x, scores, figure_file):\n",
        "    running_avg = np.zeros(len(scores))\n",
        "    for i in range(len(running_avg)):\n",
        "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
        "    plt.plot(x, running_avg)\n",
        "    plt.title('Running average of previous 100 scores')\n",
        "    plt.savefig(figure_file)"
      ],
      "metadata": {
        "id": "kQfC6kL4L2wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "env = gym.make('CartPole-v0')\n",
        "agent = Agent(alpha=1e-5, n_actions=env.action_space.n)\n",
        "n_games = 1800\n",
        "\n",
        "filename = 'cartpole_games.png'\n",
        "\n",
        "figure_file = 'plots/' + filename\n",
        "\n",
        "best_score = env.reward_range[0]\n",
        "score_history = []\n",
        "load_checkpoint = False\n",
        "\n",
        "if load_checkpoint:\n",
        "    agent.load_models()\n",
        "\n",
        "for i in range(n_games):\n",
        "    observation = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "    while not done:\n",
        "        action = agent.choose_action(observation)\n",
        "        observation_, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "        if not load_checkpoint:\n",
        "            agent.learn(observation, reward, observation_, done)\n",
        "        observation = observation_\n",
        "    score_history.append(score)\n",
        "    avg_score = np.mean(score_history[-100:])\n",
        "\n",
        "    if avg_score > best_score:\n",
        "        best_score = avg_score\n",
        "        if not load_checkpoint:\n",
        "            agent.save_models()\n",
        "\n",
        "    print('episode ', i, 'score %.1f' % score, 'avg_score %.1f' % avg_score)\n",
        "\n",
        "if not load_checkpoint:\n",
        "    x = [i+1 for i in range(n_games)]\n",
        "    plot_learning_curve(x, score_history, figure_file)"
      ],
      "metadata": {
        "id": "5jedNmESL2kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pLS2J7rBL2c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Understand categorical distribution\n",
        "agent  = Agent()\n",
        "\n",
        "state = [0.02, 0.01, -0.03, 0.04]\n",
        "state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "_, probs = agent.actor_critic(state)\n",
        "action_probabilities = tfp.distributions.Categorical(probs=probs)\n",
        "\n",
        "action = action_probabilities.sample()\n",
        "print(action)\n",
        "\n",
        "log_prob = action_probabilities.log_prob(action)\n",
        "print(log_prob)\n",
        "print(action.numpy()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfFuaBol7lHp",
        "outputId": "bbd159d3-abb7-403e-f24c-b2ed40fe2ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0], shape=(1,), dtype=int32)\n",
            "tf.Tensor([-0.6932223], shape=(1,), dtype=float32)\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(action_probabilities.probs)\n",
        "print(action_probabilities.logits)\n",
        "print(action_probabilities.batch_shape)\n",
        "print(action_probabilities.event_shape)\n",
        "\n",
        "print(action)\n",
        "print(log_prob)\n",
        "print(action.numpy()[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAyaq2_WC4nX",
        "outputId": "69c0bf51-ecda-450e-fc8c-ac7064dd38ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[0.49996245 0.50003755]], shape=(1, 2), dtype=float32)\n",
            "None\n",
            "(1,)\n",
            "()\n",
            "tf.Tensor([0], shape=(1,), dtype=int32)\n",
            "tf.Tensor([-0.6932223], shape=(1,), dtype=float32)\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7gEjFsqY2Tp6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}